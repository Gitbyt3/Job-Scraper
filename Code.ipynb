{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fa346b-ec93-472f-ac44-7407d437ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import os.path\n",
    "import os\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError                                               \n",
    "\n",
    "#Set pandas dataframe viewing options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b5080d-a692-4435-8aca-b547d15c1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to scrape linkedin\n",
    "def linkedin_scraper(url):\n",
    "\n",
    "    #URL must be formatted without the job id in the URL and with '&start={}' at the end of the URL, allowing for the .format command to work\n",
    "    chrome_driver.get(url.format(0))\n",
    "\n",
    "    #Time.sleep is used to allow enough time for the browser to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    #Locates the number of search results and converts this into how many pages there are for the search query\n",
    "    noresults = int(''.join(re.findall(r'\\d+',chrome_driver.find_element(By.XPATH,\"//div[@class='jobs-search-results-list__subtitle']\").text)))\n",
    "    nopages = math.ceil(noresults/25)\n",
    "\n",
    "    #Converts the current page into a beautiful soup object and extracts all the IDs (this is the only field not needing javascript scrolling\n",
    "    soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "    id = [x.get('data-occludable-job-id') for x in soup.find_all('li',attrs={'data-occludable-job-id':True})]\n",
    "    titles = []\n",
    "    companies = []\n",
    "    links = []\n",
    "\n",
    "    #Loops through the extracted IDs\n",
    "    #Scrolls to the position of the current ID then extracts the title, company, and link at that location\n",
    "    for x in id:\n",
    "        chrome_driver.execute_script(\"arguments[0].scrollIntoView(true);\",chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']\"))\n",
    "        titles = titles + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//a[@class='disabled ember-view job-card-container__link job-card-list__title']\").text]\n",
    "        companies = companies + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//span[@class='job-card-container__primary-description ']\").text]\n",
    "        links = links + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//a[@class='disabled ember-view job-card-container__link job-card-list__title']\").get_attribute('href')]\n",
    "\n",
    "    #If there is more than 1 page of search results runs the below code\n",
    "    if nopages > 1:\n",
    "        \n",
    "        #For each additional page, navigates to a new URL associated with the next page and extracts the data on that page\n",
    "        for i in range(1,nopages):\n",
    "            chrome_driver.get(url.format(i*25))\n",
    "            time.sleep(5)\n",
    "            soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "            newid = [x.get('data-occludable-job-id') for x in soup.find_all('li',attrs={'data-occludable-job-id':True})]\n",
    "            id = id + newid\n",
    "            for x in newid:\n",
    "                chrome_driver.execute_script(\"arguments[0].scrollIntoView(true);\",chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']\"))\n",
    "                titles = titles + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//a[@class='disabled ember-view job-card-container__link job-card-list__title']\").text]\n",
    "                companies = companies + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//span[@class='job-card-container__primary-description ']\").text]\n",
    "                links = links + [chrome_driver.find_element(By.XPATH,f\"//li[@data-occludable-job-id='{x}']//a[@class='disabled ember-view job-card-container__link job-card-list__title']\").get_attribute('href')]\n",
    "\n",
    "    #Combines the extracted data into a dataframe called 'linkedin'\n",
    "    linkedin = pd.DataFrame({'ID':id,'Title':titles,'Company':companies,'Link':links})\n",
    "    return linkedin\n",
    "\n",
    "#Initialises the Selenium chrome driver. Options are not specified but can be added as needed (code does not work in headless mode)\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "#Navigates to the LinkedIn login page and sends credentials as specified here\n",
    "chrome_driver.get(\"https://linkedin.com/uas/login\")\n",
    "time.sleep(5)\n",
    "chrome_driver.find_element(By.ID,\"username\").send_keys('INPUT_LINKEDIN_USERNAME') \n",
    "chrome_driver.find_element(By.ID,\"password\").send_keys('INPUT_LINKEDIN_PASSWORD')\n",
    "chrome_driver.find_element(By.XPATH,\"//button[@type='submit']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Runs LinkedIn scraping for four distinct keywords: 'Graduate Data', 'Intern Data', 'Graduate Economics', and 'Intern Economics'\n",
    "#Each query is limited to the past 24 hours and with the experience levels 'Intern' and 'Entry Level'\n",
    "grad_data = linkedin_scraper(\"https://www.linkedin.com/jobs/search/?distance=25&f_E=1%2C2&f_TPR=r86400&geoId=102257491&keywords=graduate%20data&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&start={}\")\n",
    "intern_data = linkedin_scraper(\"https://www.linkedin.com/jobs/search/?f_E=1%2C2&f_TPR=r86400&geoId=102257491&keywords=intern%20data&location=London%2C%20England%2C%20United%20Kingdom&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&start={}\")\n",
    "grad_econ = linkedin_scraper(\"https://www.linkedin.com/jobs/search/?f_E=1%2C2&f_TPR=r86400&geoId=102257491&keywords=graduate%20economics&location=London%2C%20England%2C%20United%20Kingdom&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start={}\")\n",
    "intern_econ = linkedin_scraper(\"https://www.linkedin.com/jobs/search/?f_E=1%2C2&f_TPR=r86400&geoId=102257491&keywords=intern%20economics&location=London%2C%20England%2C%20United%20Kingdom&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true&start={}\")\n",
    "\n",
    "#Combines the resulting dataframes into a final dataframe without duplicates\n",
    "linkedin = pd.concat([grad_data,intern_data,grad_econ,intern_econ],axis=0,ignore_index=True)\n",
    "linkedin.drop_duplicates(subset='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14560312-34a5-4f46-9d22-f2f1fe731c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n"
     ]
    }
   ],
   "source": [
    "#Define function to scrape Glassdoor\n",
    "def glassdoor_scraper(url):\n",
    "    \n",
    "    #Opens up the query of interest\n",
    "    chrome_driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Removes the cookie pop-up if there is one\n",
    "    try:\n",
    "        chrome_driver.find_element(By.XPATH,\"//button[@id='onetrust-accept-btn-handler']\").click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No pop-up')\n",
    "\n",
    "    #Locates the number of search results and converts this into how many clicks are needed to display all the available results\n",
    "    noresults = int(''.join(re.findall(r'\\d+',chrome_driver.find_element(By.XPATH,\"//h1[@data-test='search-title']\").text)))\n",
    "    noclicks = math.ceil(noresults/30)\n",
    "\n",
    "    #If there is more than 1 page of search results runs the below code\n",
    "    if noclicks > 1:\n",
    "\n",
    "        #For each additional click, clicks the 'Load More' button at the bottom of the page\n",
    "        for i in range(1,noclicks):\n",
    "            chrome_driver.find_element(By.XPATH,\"//button[@data-test='load-more']\").click()\n",
    "            time.sleep(5)\n",
    "\n",
    "    #once all the results are displayed, converts the page to a beautiful soup object and extracts all the relevant information\n",
    "    soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "    links = [x.find('a')['href'] for x in soup.find('ul',attrs={'aria-label':'Jobs List'}).find_all('li')[:-1]]\n",
    "    titles = [x.find('a').get_text() for x in soup.find('ul',attrs={'aria-label':'Jobs List'}).find_all('li')[:-1]]\n",
    "    company = [x.find('span',attrs={'class':'EmployerProfile_employerName__Xemli'}).get_text() for x in soup.find('ul',attrs={'aria-label':'Jobs List'}).find_all('li')[:-1]]\n",
    "    id = [x.get('data-jobid') for x in soup.find('ul',attrs={'aria-label':'Jobs List'}).find_all('li')[:-1]]\n",
    "\n",
    "    #Saves the resulting lists into a dataframe\n",
    "    glassdoor = pd.DataFrame({'ID':id,'Title':titles,'Company':company,'Link':links})\n",
    "    return glassdoor\n",
    "\n",
    "#Initial time.sleep is to allow space between the LinkedIn and Glassdoor scrapes\n",
    "time.sleep(10)\n",
    "\n",
    "#Navigates to the Glassdoor login page and logs in with Facebook using the below credentials\n",
    "#Window switching is to ensure the relevant browser window is in focus\n",
    "chrome_driver.get(\"https://www.glassdoor.co.uk/profile/login_input.htm\")\n",
    "time.sleep(5)\n",
    "chrome_driver.find_element(By.XPATH,\"//button[contains(@class, 'facebookWhite gd-btn center')]\").click()\n",
    "time.sleep(5)\n",
    "chrome_driver.switch_to.window(chrome_driver.window_handles[-1])\n",
    "chrome_driver.find_element(By.XPATH,\"//input[contains(@name,'email')]\").send_keys('INPUT_FACEBOOK_EMAIL')\n",
    "chrome_driver.find_element(By.XPATH,\"//input[contains(@name,'pass')]\").send_keys('INPUT_FACEBOOK_PASSWORD')\n",
    "chrome_driver.find_element(By.XPATH,\"//input[contains(@name,'pass')]\").send_keys(Keys.ENTER)\n",
    "chrome_driver.switch_to.window(chrome_driver.window_handles[0])\n",
    "time.sleep(15)\n",
    "\n",
    "#Runs the same queries as LinkedIn\n",
    "grad_data = glassdoor_scraper(\"https://www.glassdoor.co.uk/Job/london-england-graduate-data-jobs-SRCH_IL.0,14_IC2671300_KO15,28.htm?fromAge=3\")\n",
    "intern_data = glassdoor_scraper(\"https://www.glassdoor.co.uk/Job/london-england-intern-data-jobs-SRCH_IL.0,14_IC2671300_KO15,26.htm?fromAge=3\")\n",
    "grad_econ = glassdoor_scraper(\"https://www.glassdoor.co.uk/Job/london-england-graduate-economics-jobs-SRCH_IL.0,14_IC2671300_KO15,33.htm?fromAge=3\")\n",
    "intern_econ = glassdoor_scraper(\"https://www.glassdoor.co.uk/Job/london-england-intern-economics-jobs-SRCH_IL.0,14_IC2671300_KO15,31.htm?fromAge=3\")\n",
    "\n",
    "#Combines the dataframes into one big one without duplicates\n",
    "glassdoor = pd.concat([grad_data,intern_data,grad_econ,intern_econ],axis=0,ignore_index=True)\n",
    "glassdoor.drop_duplicates(subset='ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3e99af-e556-44cf-870d-fae5569b44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n",
      "No pop-up\n"
     ]
    }
   ],
   "source": [
    "#Define function to scrape Indeed\n",
    "def indeed_scraper(url):\n",
    "\n",
    "    #Opens the query of interest\n",
    "    chrome_driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    #Removes any pop-ups\n",
    "    try:\n",
    "        chrome_driver.find_element(By.XPATH,\"//button[@id='onetrust-reject-all-handler']\").click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('No pop-up')\n",
    "    try:\n",
    "        chrome_driver.find_element(By.XPATH,\"//button[@onclick='closeGoogleOnlyModal()']\").click()\n",
    "        time.sleep(3)\n",
    "        chrome_driver.find_element(By.XPATH,\"//button[@aria-label='close']\").click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('No pop-up')\n",
    "\n",
    "    #Converts the current page to a beautiful soup object and exxtracts all relevant data\n",
    "    soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "    id = [item for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'}) for item in x.find('div').get('class') if 'job' in item]\n",
    "    titles = [x.find('h2').get_text() if x.find('h2') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "    titles = [x for x in titles if x != '-']\n",
    "    company = [x.get_text() for x in soup.find_all('span',attrs={'data-testid':'company-name'})]\n",
    "    link = ['https://uk.indeed.com' + x.find('a').get('href') if x.find('a') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "    link = [x for x in link if x != '-']\n",
    "\n",
    "    #If a button to navigate to the next page of results exists, extract the data from the next pages\n",
    "    if soup.find('ul',attrs={'class':'css-1g90gv6 eu4oa1w0'}).find('a',attrs={'aria-label':'Next Page'}) is not None:\n",
    "        chrome_driver.find_element(By.XPATH,\"//a[@aria-label='Next Page']\").click()\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            chrome_driver.find_elementz(By.XPATH,\"//button[@onclick='closeGoogleOnlyModal()']\").click()\n",
    "            time.sleep(3)\n",
    "            chrome_driver.find_element(By.XPATH,\"//button[@aria-label='close']\").click()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print('No pop-up')\n",
    "        soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "        id = id + [item for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'}) for item in x.find('div').get('class') if 'job' in item]\n",
    "        titles = titles + [x.find('h2').get_text() if x.find('h2') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "        titles = [x for x in titles if x != '-']\n",
    "        company = company + [x.get_text() for x in soup.find_all('span',attrs={'data-testid':'company-name'})]\n",
    "        link = link + ['https://uk.indeed.com' + x.find('a').get('href') if x.find('a') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "        link = [x for x in link if x != '-']\n",
    "\n",
    "    #If, after the first next page press, there are still more pages, the code will keep extracting data from the next page until the next page button no longer exists\n",
    "    while soup.find('ul',attrs={'class':'css-1g90gv6 eu4oa1w0'}).find('a',attrs={'aria-label':'Next Page'}) is not None:\n",
    "        chrome_driver.find_element(By.XPATH,\"//a[@aria-label='Next Page']\").click()\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(chrome_driver.page_source,'html.parser')\n",
    "        id = id + [item for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'}) for item in x.find('div').get('class') if 'job' in item]\n",
    "        titles = titles + [x.find('h2').get_text() if x.find('h2') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "        titles = [x for x in titles if x != '-']\n",
    "        company = company + [x.get_text() for x in soup.find_all('span',attrs={'data-testid':'company-name'})]\n",
    "        link = link + ['https://uk.indeed.com' + x.find('a').get('href') if x.find('a') is not None else '-' for x in soup.find_all('li',attrs={'class':'css-5lfssm eu4oa1w0'})]\n",
    "        link = [x for x in link if x != '-']\n",
    "\n",
    "    #Saves the data as a dataframe\n",
    "    indeed = pd.DataFrame({'ID':id,'Title':titles,'Company':company,'Link':link})\n",
    "    return indeed\n",
    "\n",
    "#Time.sleep to give enough time between Glassdoor and Indeed\n",
    "time.sleep(10)\n",
    "\n",
    "#Runs the same queries but on Indeed\n",
    "grad_data = indeed_scraper(\"https://uk.indeed.com/jobs?q=graduate+data&l=London%2C+Greater+London&fromage=1\")\n",
    "intern_data = indeed_scraper(\"https://uk.indeed.com/jobs?q=intern+data&l=London%2C+Greater+London&fromage=1\")\n",
    "grad_econ = indeed_scraper(\"https://uk.indeed.com/jobs?q=graduate+economics&l=London%2C+Greater+London&fromage=1\")\n",
    "intern_econ = indeed_scraper(\"https://uk.indeed.com/jobs?q=intern+economics&l=London%2C+Greater+London&fromage=1\")\n",
    "\n",
    "#Saves the outputs into one dataframe without duplicates\n",
    "indeed = pd.concat([grad_data,intern_data,grad_econ,intern_econ],axis=0,ignore_index=True)\n",
    "indeed.drop_duplicates(subset='ID',inplace=True)\n",
    "\n",
    "#Closes the chrome driver\n",
    "chrome_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2aff04-a35c-4711-a646-d42bee978d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activates the Google Sheets API credentials\n",
    "#Requires a Google Cloud account and for a project to be open with OAuth credentials\n",
    "def credentials():\n",
    "    scopes = \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "    credentials = None\n",
    "    if os.path.exists(\"API_Files/token.json\"):\n",
    "        credentials = Credentials.from_authorized_user_file(\"API_Files/token.json\",scopes)\n",
    "    if not credentials or not credentials.valid:\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"API_Files/credentials.json\",scopes)\n",
    "            credentials = flow.run_local_server(port=0)\n",
    "        with open(\"API_Files/token.json\",\"w\") as token:\n",
    "            token.write(credentials.to_json())\n",
    "    return credentials\n",
    "credentials = credentials()\n",
    "\n",
    "#Builds the Google Sheets API service\n",
    "service = build(\"sheets\",\"v4\",credentials=credentials)\n",
    "\n",
    "#For each data source this is the order of operations\n",
    "# 1. Pull existing data from the relevant tab in the Google Sheet\n",
    "# 2. Filters the scraped data to only include the data not already on the Google Sheet (uses the 'ID' column)\n",
    "# 3. Concatenates the existing data with the new data into one dataframe, ready to be uploaded\n",
    "# 4. Updates the relevant tabs with the complete dataframes in the Google Sheet\n",
    "\n",
    "request = service.spreadsheets().values().get(spreadsheetId=\"SPREADSHEET_ID\",range='LinkedIn').execute()\n",
    "current_linkedin = pd.DataFrame(data=request['values'][1:],columns=request['values'][0])\n",
    "linkedin_to_upload = linkedin.loc[~linkedin['ID'].isin(current_linkedin['ID'])]\n",
    "linkedin = pd.concat([current_linkedin,linkedin_to_upload],axis=0,ignore_index=True)\n",
    "body = {'values':[linkedin.columns.tolist()] + linkedin.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEET_ID\",range='LinkedIn',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "\n",
    "request = service.spreadsheets().values().get(spreadsheetId=\"SPREADSHEET_ID\",range='Glassdoor').execute()\n",
    "current_glassdoor = pd.DataFrame(data=request['values'][1:],columns=request['values'][0])\n",
    "glassdoor_to_upload = glassdoor.loc[~glassdoor['ID'].isin(current_glassdoor['ID'])]\n",
    "glassdoor = pd.concat([current_glassdoor,glassdoor_to_upload],axis=0,ignore_index=True)\n",
    "body = {'values':[glassdoor.columns.tolist()] + glassdoor.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEET_ID\",range='Glassdoor',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "\n",
    "request = service.spreadsheets().values().get(spreadsheetId=\"SPREADSHEET_ID\",range='Indeed').execute()\n",
    "current_indeed = pd.DataFrame(data=request['values'][1:],columns=request['values'][0])\n",
    "indeed_to_upload = indeed.loc[~indeed['ID'].isin(current_indeed['ID'])]\n",
    "indeed = pd.concat([current_indeed,indeed_to_upload],axis=0,ignore_index=True)\n",
    "body = {'values':[indeed.columns.tolist()] + indeed.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEET_ID\",range='Indeed',valueInputOption=\"USER_ENTERED\",body=body).execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
